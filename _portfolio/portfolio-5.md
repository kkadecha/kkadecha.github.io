---
title: "Integration of Voice, Vision, and Autonomous Control Systems for LIMO ROS2 Robot"
excerpt: "Developed a Proof of Concept (POC) for human-robot interaction using the Agilex LIMO robot. The system enables the robot to understand and execute human commands through speech-to-text (Whisper AI), real-time object detection (YOLO), and autonomous navigation (ROS2). The key challenge was achieving accurate navigation towards a specific object based on verbal instructions. This project showcases the potential of AI-driven robotics in intuitive human-robot interaction.<br /><br /><b>Skills : Python, ROS2, Mobile Robotics, Computer Vision, YOLO, Large Language Models (LLM), OLLAMA, Object Detection, Speech-To-Text, Autonoumous Navigation of Mobile Robot, Machine Learning & Deep Learning</b>"
collection: portfolio
---

I developed a Proof of Concept (POC) for human-robot interaction, enabling the Agilex LIMO robot to interpret and execute human commands seamlessly. The system integrates multiple AI and robotics technologies to achieve intuitive and autonomous navigation based on verbal instructions.<br />

Key Features & Technologies Used:<br />

ðŸ”¹ Speech-to-Text Processing â€“ Implemented Whisper AI to convert spoken instructions into text for real-time command interpretation.<br />
ðŸ”¹ Real-Time Object Detection â€“ Used YOLO (You Only Look Once) for accurate object recognition, enabling the robot to identify specific targets.<br />
ðŸ”¹ Autonomous Navigation â€“ Integrated ROS2 (Robot Operating System 2) for path planning and obstacle avoidance, ensuring smooth movement.<br />
ðŸ”¹ Command Execution Pipeline â€“ Combined natural language processing with robotic perception to dynamically control the robotâ€™s actions.<br />

Key Challenge & Solution:<br />

One of the biggest challenges was enabling the LIMO robot to autonomously navigate towards a specific object based on verbal commands. Fine-tuning the object detection system and optimizing navigation algorithms were critical to improving accuracy and real-time performance.<br />

This project demonstrates the potential of human-robot interaction, bridging the gap between natural communication and autonomous robotic execution. It opens doors for voice-driven robotics applications, such as assistive robots, warehouse automation, and smart mobility solutions.<br />

Would love to connect with robotics, AI, and automation enthusiasts to discuss innovations in human-robot collaboration! ðŸš€ 
<br /><br />
<b>Skills : Python, ROS2, Mobile Robotics, Computer Vision, YOLO, Large Language Models (LLM), OLLAMA, Object Detection, Speech-To-Text, Autonoumous Navigation of Mobile Robot, Machine Learning & Deep Learning</b>
